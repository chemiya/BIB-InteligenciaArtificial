{"cells":[{"cell_type":"markdown","source":["# Deep Convolutional Q-Learning para PacMan"],"metadata":{"id":"EAiHVEoWHy_D"}},{"cell_type":"markdown","source":["## Part 0 - Instalar paquetes e importar librerias"],"metadata":{"id":"tjO1aK3Ddjs5"}},{"cell_type":"markdown","source":["### Instalar Gymnasium"],"metadata":{"id":"NwdRB-ZLdrAV"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"dbnq3XpoKa_7","colab":{"base_uri":"https://localhost:8080/"},"outputId":"001e0941-a9c4-4642-a372-52156cd740d6","executionInfo":{"status":"ok","timestamp":1723910972447,"user_tz":-120,"elapsed":83534,"user":{"displayName":"chema tfg","userId":"03198431984494101568"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gymnasium\n","  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n","Collecting farama-notifications>=0.0.1 (from gymnasium)\n","  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n","Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n","Installing collected packages: farama-notifications, gymnasium\n","Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n","Requirement already satisfied: gymnasium[accept-rom-license,atari] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (4.12.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[accept-rom-license,atari]) (0.0.4)\n","Collecting shimmy<1.0,>=0.1.0 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari])\n","  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n","Collecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari])\n","  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (8.1.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (4.66.5)\n","Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari])\n","  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting ale-py~=0.8.1 (from shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari])\n","  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]<1.0,>=0.1.0; extra == \"atari\"->gymnasium[accept-rom-license,atari]) (6.4.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gymnasium[accept-rom-license,atari]) (2024.7.4)\n","Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n","Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n","Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m77.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: AutoROM.accept-rom-license\n","  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446660 sha256=12163d94b25e9c8ff15661af0f4a51e072521291bf1097cdc5eb6cc99e97403c\n","  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n","Successfully built AutoROM.accept-rom-license\n","Installing collected packages: ale-py, shimmy, AutoROM.accept-rom-license, autorom\n","Successfully installed AutoROM.accept-rom-license-0.6.1 ale-py-0.8.1 autorom-0.4.2 shimmy-0.2.1\n","Reading package lists... Done\n","Building dependency tree... Done\n","Reading state information... Done\n","The following additional packages will be installed:\n","  swig4.0\n","Suggested packages:\n","  swig-doc swig-examples swig4.0-examples swig4.0-doc\n","The following NEW packages will be installed:\n","  swig swig4.0\n","0 upgraded, 2 newly installed, 0 to remove and 45 not upgraded.\n","Need to get 1,116 kB of archives.\n","After this operation, 5,542 kB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig4.0 amd64 4.0.2-1ubuntu1 [1,110 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 swig all 4.0.2-1ubuntu1 [5,632 B]\n","Fetched 1,116 kB in 2s (612 kB/s)\n","Selecting previously unselected package swig4.0.\n","(Reading database ... 123594 files and directories currently installed.)\n","Preparing to unpack .../swig4.0_4.0.2-1ubuntu1_amd64.deb ...\n","Unpacking swig4.0 (4.0.2-1ubuntu1) ...\n","Selecting previously unselected package swig.\n","Preparing to unpack .../swig_4.0.2-1ubuntu1_all.deb ...\n","Unpacking swig (4.0.2-1ubuntu1) ...\n","Setting up swig4.0 (4.0.2-1ubuntu1) ...\n","Setting up swig (4.0.2-1ubuntu1) ...\n","Processing triggers for man-db (2.10.2-1) ...\n","Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n","Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n","  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.0)\n","Collecting swig==4.* (from gymnasium[box2d])\n","  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (3.6 kB)\n","Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: box2d-py\n","  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2349144 sha256=9fe077403b2654e89dde21e68283a04d9650a4084f4656e6c006347be7f13c9f\n","  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n","Successfully built box2d-py\n","Installing collected packages: swig, box2d-py\n","Successfully installed box2d-py-2.3.5 swig-4.2.1\n"]}],"source":["!pip install gymnasium\n","!pip install \"gymnasium[atari, accept-rom-license]\"\n","!apt-get install -y swig\n","!pip install gymnasium[box2d]"]},{"cell_type":"markdown","source":["### Importar las librerias"],"metadata":{"id":"H-wes4LZdxdd"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"Ho_25-9_9qnu","executionInfo":{"status":"ok","timestamp":1723910977311,"user_tz":-120,"elapsed":4869,"user":{"displayName":"chema tfg","userId":"03198431984494101568"}}},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from collections import deque\n","from torch.utils.data import DataLoader, TensorDataset"]},{"cell_type":"markdown","source":["## Part 1 - Building the AI"],"metadata":{"id":"m7wa0ft8e3M_"}},{"cell_type":"markdown","source":["### Crear la arquitectura de la red neuronal"],"metadata":{"id":"dlYVpVdHe-i6"}},{"cell_type":"code","source":["class Network(nn.Module):\n","\n","  def __init__(self, action_size, seed=42):\n","    super(Network, self).__init__()\n","    # Establece una semilla para la reproducibilidad.\n","    self.seed = torch.manual_seed(seed)\n","\n","    # Define las capas de la red neuronal convolucional.\n","    self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)  # Primera capa convolucional: convierte 3 canales (RGB) en 32 canales, con un kernel de 8x8 y stride de 4.\n","    self.bn1 = nn.BatchNorm2d(32)  # Normalización por lotes para la primera capa convolucional.\n","\n","    self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)  # Segunda capa convolucional: convierte 32 canales en 64 canales, con un kernel de 4x4 y stride de 2.\n","    self.bn2 = nn.BatchNorm2d(64)  # Normalización por lotes para la segunda capa convolucional.\n","\n","    self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)  # Tercera capa convolucional: mantiene 64 canales con un kernel de 3x3 y stride de 1.\n","    self.bn3 = nn.BatchNorm2d(64)  # Normalización por lotes para la tercera capa convolucional.\n","\n","    self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1)  # Cuarta capa convolucional: convierte 64 canales en 128 canales, con un kernel de 3x3 y stride de 1.\n","    self.bn4 = nn.BatchNorm2d(128)  # Normalización por lotes para la cuarta capa convolucional.\n","\n","    # Define las capas totalmente conectadas.\n","    self.fc1 = nn.Linear(10 * 10 * 128, 512)  # Primera capa totalmente conectada: toma la salida de la última capa convolucional (ajustada para tener dimensiones 10x10x128) y la convierte en un vector de 512 dimensiones.\n","    self.fc2 = nn.Linear(512, 256)  # Segunda capa totalmente conectada: convierte 512 dimensiones a 256 dimensiones.\n","    self.fc3 = nn.Linear(256, action_size)  # Tercera capa totalmente conectada: convierte 256 dimensiones a `action_size`, el número de acciones posibles.\n","\n","  def forward(self, state):\n","    # Define el paso hacia adelante de la red neuronal.\n","\n","    # Propaga el estado a través de las capas convolucionales y de normalización por lotes con la función de activación ReLU.\n","    x = F.relu(self.bn1(self.conv1(state)))\n","    x = F.relu(self.bn2(self.conv2(x)))\n","    x = F.relu(self.bn3(self.conv3(x)))\n","    x = F.relu(self.bn4(self.conv4(x)))\n","\n","    # Aplana la salida de las capas convolucionales para que pueda ser pasada a las capas totalmente conectadas.\n","    x = x.view(x.size(0), -1)\n","\n","    # Propaga la salida a través de las capas totalmente conectadas con la función de activación ReLU.\n","    x = F.relu(self.fc1(x))\n","    x = F.relu(self.fc2(x))\n","\n","    # Devuelve las salidas de la última capa totalmente conectada, que representa los valores de acción.\n","    return self.fc3(x)"],"metadata":{"id":"4ZW9ybp2S-op","executionInfo":{"status":"ok","timestamp":1723910977311,"user_tz":-120,"elapsed":4,"user":{"displayName":"chema tfg","userId":"03198431984494101568"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["## Part 2 - Entrenamiento"],"metadata":{"id":"rUvCfE_mhwo2"}},{"cell_type":"markdown","source":["### Estableciendo el entorno"],"metadata":{"id":"WWCDPF22lkwc"}},{"cell_type":"code","source":["import gymnasium as gym\n","env = gym.make('MsPacmanDeterministic-v0', full_action_space = False)\n","state_shape = env.observation_space.shape\n","state_size = env.observation_space.shape[0]\n","number_actions = env.action_space.n\n","print('State shape: ', state_shape)\n","print('State size: ', state_size)\n","print('Number of actions: ', number_actions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qYLnz2Z6PqeU","outputId":"5ca7d3b3-0656-49b1-eeef-be69019f298f","executionInfo":{"status":"ok","timestamp":1723910979033,"user_tz":-120,"elapsed":1725,"user":{"displayName":"chema tfg","userId":"03198431984494101568"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment MsPacmanDeterministic-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  logger.deprecation(\n"]},{"output_type":"stream","name":"stdout","text":["State shape:  (210, 160, 3)\n","State size:  210\n","Number of actions:  9\n"]}]},{"cell_type":"markdown","source":["### Inicilizar hiperparametros"],"metadata":{"id":"Bx6IdX3ciDqH"}},{"cell_type":"code","source":["learning_rate = 5e-4\n","minibatch_size = 64\n","discount_factor = 0.99"],"metadata":{"id":"pQHWLxOwrb0J","executionInfo":{"status":"ok","timestamp":1723910979033,"user_tz":-120,"elapsed":4,"user":{"displayName":"chema tfg","userId":"03198431984494101568"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ac57a30e-965f-4cf1-9098-10650a90fca0"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","source":["### Preprocesar los frames"],"metadata":{"id":"U2bDShIEkA5V"}},{"cell_type":"code","source":["from PIL import Image\n","from torchvision import transforms\n","\n","def preprocess_frame(frame):\n","  # Convierte el array de numpy `frame` a una imagen de PIL.\n","  frame = Image.fromarray(frame)\n","\n","  # Define una secuencia de transformaciones a aplicar a la imagen:\n","  # 1. Resize: Redimensiona la imagen a un tamaño de 128x128 píxeles.\n","  # 2. ToTensor: Convierte la imagen a un tensor de PyTorch (normaliza los valores de píxeles a [0, 1] y cambia el formato a [C, H, W]).\n","  preprocess = transforms.Compose([transforms.Resize((128, 128)), transforms.ToTensor()])\n","\n","  # Aplica las transformaciones definidas a la imagen y añade una dimensión extra en la posición 0 para crear un batch de tamaño 1.\n","  return preprocess(frame).unsqueeze(0)\n"],"metadata":{"id":"VporQhgMDygL","executionInfo":{"status":"ok","timestamp":1723910980444,"user_tz":-120,"elapsed":1414,"user":{"displayName":"chema tfg","userId":"03198431984494101568"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["### Implementar la DCQN class"],"metadata":{"id":"imMdSO-HAWra"}},{"cell_type":"code","source":["class Agent():\n","\n","  def __init__(self, action_size):\n","    # Determina el dispositivo de computación (GPU si está disponible, de lo contrario CPU).\n","    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Número de acciones posibles en el entorno.\n","    self.action_size = action_size\n","\n","    # Red neuronal local para estimar los valores Q.\n","    self.local_qnetwork = Network(action_size).to(self.device)\n","\n","    # Red neuronal objetivo para estabilizar el entrenamiento.\n","    self.target_qnetwork = Network(action_size).to(self.device)\n","\n","    # Optimizador para actualizar los pesos de la red neuronal.\n","    self.optimizer = optim.Adam(self.local_qnetwork.parameters(), lr=learning_rate)\n","\n","    # Memoria para almacenar las experiencias de los episodios.\n","    self.memory = deque(maxlen=10000)\n","\n","  def step(self, state, action, reward, next_state, done):\n","    # Preprocesa las imágenes de estado y próximo estado.\n","    state = preprocess_frame(state)\n","    next_state = preprocess_frame(next_state)\n","\n","    # Almacena la experiencia en la memoria.\n","    self.memory.append((state, action, reward, next_state, done))\n","\n","    # Si la memoria tiene suficientes experiencias, realiza el aprendizaje.\n","    if len(self.memory) > minibatch_size:\n","      experiences = random.sample(self.memory, k=minibatch_size)\n","      self.learn(experiences, discount_factor)\n","\n","  def act(self, state, epsilon=0.):\n","    # Preprocesa el estado.\n","    state = preprocess_frame(state).to(self.device)\n","\n","    # Pone la red neuronal en modo evaluación para evitar que se actualicen los pesos durante la inferencia.\n","    self.local_qnetwork.eval()\n","\n","    with torch.no_grad():\n","      # Obtiene los valores Q de la red neuronal local para el estado dado.\n","      action_values = self.local_qnetwork(state)\n","\n","    # Restaura la red neuronal al modo de entrenamiento.\n","    self.local_qnetwork.train()\n","\n","    # Decisión de acción basada en epsilon-greedy.\n","    if random.random() > epsilon:\n","      # Selecciona la acción con el valor Q más alto (explotación).\n","      return np.argmax(action_values.cpu().data.numpy())\n","    else:\n","      # Selecciona una acción aleatoria (exploración).\n","      return random.choice(np.arange(self.action_size))\n","\n","  def learn(self, experiences, discount_factor):\n","    # Extrae las experiencias de la muestra de minibatch.\n","    states, actions, rewards, next_states, dones = zip(*experiences)\n","\n","    # Convierte las experiencias en tensores y mueve los tensores al dispositivo de computación.\n","    states = torch.from_numpy(np.vstack(states)).float().to(self.device)\n","    actions = torch.from_numpy(np.vstack(actions)).long().to(self.device)\n","    rewards = torch.from_numpy(np.vstack(rewards)).float().to(self.device)\n","    next_states = torch.from_numpy(np.vstack(next_states)).float().to(self.device)\n","    dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).float().to(self.device)\n","\n","    # Calcula los valores Q objetivo utilizando la red neuronal objetivo.\n","    next_q_targets = self.target_qnetwork(next_states).detach().max(1)[0].unsqueeze(1)\n","    q_targets = rewards + discount_factor * next_q_targets * (1 - dones)\n","\n","    # Obtiene los valores Q esperados para las acciones tomadas utilizando la red neuronal local.\n","    q_expected = self.local_qnetwork(states).gather(1, actions)\n","\n","    # Calcula la pérdida (error cuadrático medio) entre los valores Q esperados y los valores Q objetivos.\n","    loss = F.mse_loss(q_expected, q_targets)\n","\n","    # Optimiza la red neuronal.\n","    self.optimizer.zero_grad()\n","    loss.backward()\n","    self.optimizer.step()"],"metadata":{"id":"N7dwS2q2wupJ","executionInfo":{"status":"ok","timestamp":1723910981043,"user_tz":-120,"elapsed":602,"user":{"displayName":"chema tfg","userId":"03198431984494101568"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c10009aa-9cfc-4acc-c369-916d7e985efe"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","source":["### Inicializar el DCQN agent"],"metadata":{"id":"yUg95iBpAwII"}},{"cell_type":"code","source":["agent = Agent(number_actions)"],"metadata":{"id":"eJJB1Gr6IKnC","executionInfo":{"status":"ok","timestamp":1723910981044,"user_tz":-120,"elapsed":5,"user":{"displayName":"chema tfg","userId":"03198431984494101568"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["### Entrenar el DCQN agent"],"metadata":{"id":"CK6Zt_gNmHvm"}},{"cell_type":"code","source":["# Parámetros de entrenamiento\n","number_episodes = 200  # Número total de episodios para entrenar el agente\n","maximum_number_timesteps_per_episode = 10000  # Número máximo de pasos por episodio\n","epsilon_starting_value = 1.0  # Valor inicial de epsilon para la política epsilon-greedy\n","epsilon_ending_value = 0.01  # Valor final de epsilon\n","epsilon_decay_value = 0.995  # Tasa de decaimiento de epsilon\n","epsilon = epsilon_starting_value  # Valor actual de epsilon\n","scores_on_100_episodes = deque(maxlen=100)  # Cola para almacenar las puntuaciones de los últimos 100 episodios\n","\n","# Bucle principal de entrenamiento\n","for episode in range(1, number_episodes + 1):\n","  # Inicialización al inicio de un nuevo episodio\n","  state, _ = env.reset()  # Restablece el entorno y obtiene el estado inicial\n","  score = 0  # Inicializa la puntuación del episodio\n","\n","  # Bucle para cada paso en el episodio\n","  for t in range(maximum_number_timesteps_per_episode):\n","    action = agent.act(state, epsilon)  # Selecciona una acción usando la política epsilon-greedy\n","    next_state, reward, done, _, _ = env.step(action)  # Ejecuta la acción en el entorno\n","    agent.step(state, action, reward, next_state, done)  # Almacena la experiencia y actualiza la red\n","    state = next_state  # Actualiza el estado actual\n","    score += reward  # Acumula la recompensa del episodio\n","    if done:\n","      break  # Termina el episodio si el entorno indica que ha terminado\n","\n","  # Almacena la puntuación del episodio en la cola de puntuaciones\n","  scores_on_100_episodes.append(score)\n","\n","  # Actualiza epsilon (estrategia de exploración)\n","  epsilon = max(epsilon_ending_value, epsilon_decay_value * epsilon)\n","\n","  # Imprime el progreso del entrenamiento\n","  print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)), end=\"\")\n","\n","  # Imprime información adicional cada 100 episodios\n","  if episode % 100 == 0:\n","    print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_on_100_episodes)))\n","\n","  # Verifica si se ha resuelto el entorno\n","  if np.mean(scores_on_100_episodes) >= 500.0:\n","    print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(episode - 100, np.mean(scores_on_100_episodes)))\n","    torch.save(agent.local_qnetwork.state_dict(), 'checkpoint.pth')  # Guarda el estado de la red neuronal\n","    break  # Sale del bucle si se ha resuelto el entorno\n"],"metadata":{"id":"XvBh4TLjIfz2","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"bf178ae1-6109-431e-81b9-19f70f9aca56","executionInfo":{"status":"error","timestamp":1723912089181,"user_tz":-120,"elapsed":1108141,"user":{"displayName":"chema tfg","userId":"03198431984494101568"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode 92\tAverage Score: 304.46"]},{"output_type":"stream","name":"stderr","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-9-0194ce951295>\", line 18, in <cell line: 11>\n","    action = agent.act(state, epsilon)  # Selecciona una acción usando la política epsilon-greedy\n","  File \"<ipython-input-7-19f11da9187a>\", line 37, in act\n","    state = preprocess_frame(state).to(self.device)\n","  File \"<ipython-input-6-87306cb65438>\", line 14, in preprocess_frame\n","    return preprocess(frame).unsqueeze(0)\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n","    img = t(img)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\", line 354, in forward\n","    return F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\", line 468, in resize\n","    return F_pil.resize(img, size=output_size, interpolation=pil_interpolation)\n","  File \"/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py\", line 250, in resize\n","    return img.resize(tuple(size[::-1]), interpolation)\n","  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 2192, in resize\n","    return self._new(self.im.resize(size, resample, box))\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n","    module = getmodule(object, filename)\n","  File \"/usr/lib/python3.10/inspect.py\", line 869, in getmodule\n","    if ismodule(module) and hasattr(module, '__file__'):\n","KeyboardInterrupt\n"]},{"output_type":"error","ename":"TypeError","evalue":"object of type 'NoneType' has no len()","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m<ipython-input-9-0194ce951295>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaximum_number_timesteps_per_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Selecciona una acción usando la política epsilon-greedy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ejecuta la acción en el entorno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-19f11da9187a>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# Preprocesa el estado.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-87306cb65438>\u001b[0m in \u001b[0;36mpreprocess_frame\u001b[0;34m(frame)\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;31m# Aplica las transformaciones definidas a la imagen y añade una dimensión extra en la posición 0 para crear un batch de tamaño 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/_functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2192\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"]}]},{"cell_type":"markdown","source":["## Part 3 - Ver resultados"],"metadata":{"id":"-0WhhBV8nQdf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cb9nVvU2Okhk","executionInfo":{"status":"aborted","timestamp":1723912089182,"user_tz":-120,"elapsed":5,"user":{"displayName":"chema tfg","userId":"03198431984494101568"}}},"outputs":[],"source":["import glob\n","import io\n","import base64\n","import imageio\n","from IPython.display import HTML, display\n","from gym.wrappers.monitoring.video_recorder import VideoRecorder\n","\n","# Crear el video\n","def show_video_of_model(agent, env_name):\n","    env = gym.make(env_name, render_mode='rgb_array')\n","    state, _ = env.reset()\n","    done = False\n","    frames = []\n","    while not done:\n","        frame = env.render()\n","        frames.append(frame)\n","        action = agent.act(state)\n","        state, reward, done, _, _ = env.step(action)\n","    env.close()\n","    imageio.mimsave('video.mp4', frames, fps=30)\n","\n","show_video_of_model(agent, 'MsPacmanDeterministic-v0')\n","\n","def show_video():\n","    mp4list = glob.glob('*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = mp4list[0]\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        display(HTML(data='''<video alt=\"test\" autoplay\n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    else:\n","        print(\"Could not find video\")\n","\n","show_video()"]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}