{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"3W0v5bQEGnoA"},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","from collections import deque"]},{"cell_type":"code","source":["# Establecer hiperparámetros\n","state_dim = env.observation_space.shape[0]\n","action_dim = env.action_space.shape[0]\n","hidden_dim = 256\n","actor_lr = 3e-4\n","critic_lr = 3e-4\n","alpha_lr = 3e-4\n","gamma = 0.99\n","tau = 0.005\n","buffer_size = 1e6\n","batch_size = 128\n","alpha = 0.2  # Entropy coefficient"],"metadata":{"id":"zmXH_WAQGrX7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ReplayBuffer:\n","    \"\"\"\n","    Clase para almacenar y gestionar una memoria de experiencia para el aprendizaje por refuerzo.\n","    Utiliza un buffer de repetición para almacenar transiciones y muestrea aleatoriamente de este buffer para el entrenamiento.\n","    \"\"\"\n","\n","    def __init__(self, capacity):\n","        \"\"\"\n","        Inicializa el ReplayBuffer con una capacidad máxima.\n","\n","        :param capacity: Capacidad máxima del buffer de repetición.\n","        \"\"\"\n","        self.buffer = deque(maxlen=int(capacity))  # Inicializa una deque con una capacidad máxima dada.\n","\n","    def push(self, state, action, reward, next_state, done):\n","        \"\"\"\n","        Añade una transición (estado, acción, recompensa, siguiente estado, terminado) al buffer de repetición.\n","\n","        :param state: Estado actual del entorno.\n","        :param action: Acción tomada en el estado actual.\n","        :param reward: Recompensa obtenida después de tomar la acción.\n","        :param next_state: Estado del entorno después de tomar la acción.\n","        :param done: Indicador booleano que indica si el episodio ha terminado.\n","        \"\"\"\n","        self.buffer.append((state, action, reward, next_state, done))  # Añade la transición al buffer.\n","\n","    def sample(self, batch_size):\n","        \"\"\"\n","        Muestra aleatoriamente un batch de transiciones del buffer de repetición.\n","\n","        :param batch_size: Tamaño del batch a muestrear.\n","        :return: Tupla de arrays numpy que contienen estados, acciones, recompensas, siguientes estados y si el episodio ha terminado.\n","        \"\"\"\n","        # Extrae las transiciones del buffer de forma aleatoria.\n","        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n","        # Convierte las transiciones a arrays numpy y retorna.\n","        return np.array(state), np.array(action), np.array(reward, dtype=np.float32), np.array(next_state), np.array(done, dtype=np.float32)\n","\n","    def __len__(self):\n","        \"\"\"\n","        Retorna el número actual de transiciones almacenadas en el buffer.\n","\n","        :return: El tamaño del buffer.\n","        \"\"\"\n","        return len(self.buffer)  # Retorna el tamaño actual del buffer de repetición.\n"],"metadata":{"id":"1TAnyduhGx-k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Actor(nn.Module):\n","    \"\"\"\n","    Clase que define el modelo de un actor en un algoritmo de aprendizaje por refuerzo basado en políticas,\n","    como el Proximal Policy Optimization (PPO) o el Actor-Critic.\n","    \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"\n","        Inicializa la red neuronal del actor.\n","        Define las capas de la red: dos capas completamente conectadas para la extracción de características,\n","        y dos capas de salida para calcular la media y el logaritmo de la desviación estándar de la distribución de la política.\n","        \"\"\"\n","        super(Actor, self).__init__()\n","        # Capa completamente conectada que toma el estado como entrada y produce una representación oculta.\n","        self.fc1 = nn.Linear(state_dim, hidden_dim)\n","        # Segunda capa completamente conectada para aumentar la capacidad de la red.\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        # Capa de salida que calcula la media de la distribución de la política.\n","        self.mean = nn.Linear(hidden_dim, action_dim)\n","        # Capa de salida que calcula el logaritmo de la desviación estándar de la distribución de la política.\n","        self.log_std = nn.Linear(hidden_dim, action_dim)\n","\n","    def forward(self, state):\n","        \"\"\"\n","        Propaga el estado a través de la red para obtener la media y el logaritmo de la desviación estándar\n","        de la distribución de la política.\n","\n","        :param state: Tensor de entrada que representa el estado del entorno.\n","        :return: media y logaritmo de la desviación estándar de la distribución de la política.\n","        \"\"\"\n","        x = torch.relu(self.fc1(state))  # Aplicar ReLU después de la primera capa oculta.\n","        x = torch.relu(self.fc2(x))      # Aplicar ReLU después de la segunda capa oculta.\n","        mean = self.mean(x)             # Calcular la media de la distribución de la política.\n","        log_std = self.log_std(x)       # Calcular el logaritmo de la desviación estándar de la distribución de la política.\n","        log_std = torch.clamp(log_std, min=-20, max=2)  # Limitar el rango de los valores de log_std para estabilidad numérica.\n","        return mean, log_std\n","\n","    def sample(self, state):\n","        \"\"\"\n","        Muestra una acción basada en el estado actual utilizando la distribución de la política.\n","\n","        :param state: Tensor de entrada que representa el estado del entorno.\n","        :return: Acción muestreada utilizando la distribución de la política.\n","        \"\"\"\n","        mean, log_std = self.forward(state)  # Obtener la media y log_std del estado actual.\n","        std = log_std.exp()  # Calcular la desviación estándar a partir del logaritmo de la desviación estándar.\n","        normal = torch.distributions.Normal(mean, std)  # Crear una distribución normal con media y desviación estándar calculadas.\n","        x_t = normal.rsample()  # Muestra una acción usando el truco de reparametrización (mean + std * N(0,1)).\n","        action = torch.tanh(x_t)  # Aplicar la función tanh para asegurar que la acción esté en el rango [-1, 1].\n","        return action\n"],"metadata":{"id":"VK9RO0TpG1oe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Critic(nn.Module):\n","    \"\"\"\n","    Clase que define el modelo del crítico en un algoritmo de aprendizaje por refuerzo basado en métodos Actor-Critic.\n","    El crítico estima el valor de una acción en un estado dado.\n","    \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"\n","        Inicializa la red neuronal del crítico.\n","        Define las capas de la red: una capa para combinar estado y acción, dos capas ocultas y una capa de salida\n","        que estima el valor del estado-acción.\n","        \"\"\"\n","        super(Critic, self).__init__()\n","        # Capa completamente conectada que toma la concatenación del estado y la acción como entrada y produce una representación oculta.\n","        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n","        # Segunda capa completamente conectada para aumentar la capacidad de la red.\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        # Capa de salida que produce una única estimación del valor del estado-acción.\n","        self.fc3 = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state, action):\n","        \"\"\"\n","        Propaga el estado y la acción a través de la red para obtener la estimación del valor del estado-acción.\n","\n","        :param state: Tensor que representa el estado del entorno.\n","        :param action: Tensor que representa la acción tomada.\n","        :return: Valor estimado del estado-acción.\n","        \"\"\"\n","        # Concatenar el estado y la acción para formar la entrada completa para el crítico.\n","        x = torch.cat([state, action], dim=1)\n","        # Aplicar ReLU después de la primera capa oculta.\n","        x = torch.relu(self.fc1(x))\n","        # Aplicar ReLU después de la segunda capa oculta.\n","        x = torch.relu(self.fc2(x))\n","        # Obtener la estimación del valor del estado-acción.\n","        x = self.fc3(x)\n","        return x\n"],"metadata":{"id":"6q0BhzJAG3er"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SACAgent:\n","    \"\"\"\n","    Clase que define un agente de Aprendizaje por Refuerzo Basado en Actor-Critic con Entropía Suave (Soft Actor-Critic, SAC).\n","    SAC es un algoritmo que maximiza la recompensa esperada mientras también maximiza la entropía para explorar más.\n","    \"\"\"\n","\n","    def __init__(self):\n","        \"\"\"\n","        Inicializa el agente SAC, creando redes para el actor y dos críticos (con sus redes objetivo),\n","        optimizadores y un buffer de repetición.\n","        \"\"\"\n","        # Inicialización de la red del actor.\n","        self.actor = Actor()\n","        # Inicialización de dos redes críticas (Critic) para la estimación de Q-values.\n","        self.critic_1 = Critic()\n","        self.critic_2 = Critic()\n","        # Inicialización de las redes críticas objetivo (target) para la actualización suave.\n","        self.target_critic_1 = Critic()\n","        self.target_critic_2 = Critic()\n","        # Copia los parámetros de las redes críticas a las redes objetivo.\n","        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n","        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n","        # Inicialización de los optimizadores para el actor y los críticos.\n","        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n","        self.critic_1_optimizer = optim.Adam(self.critic_1.parameters(), lr=critic_lr)\n","        self.critic_2_optimizer = optim.Adam(self.critic_2.parameters(), lr=critic_lr)\n","        # Inicialización del buffer de repetición para almacenar las transiciones.\n","        self.replay_buffer = ReplayBuffer(buffer_size)\n","\n","    def select_action(self, state):\n","        \"\"\"\n","        Selecciona una acción basada en el estado dado utilizando el actor.\n","\n","        :param state: Estado actual del entorno.\n","        :return: Acción seleccionada por el actor.\n","        \"\"\"\n","        # Convierte el estado a un tensor y añade una dimensión extra para el batch.\n","        state = torch.FloatTensor(state).unsqueeze(0)\n","        # Usa el actor para muestrear una acción.\n","        action = self.actor.sample(state)\n","        # Retorna la acción como un array de numpy.\n","        return action.detach().numpy()[0]\n","\n","    def update(self, batch_size, gamma=gamma, tau=tau, alpha=alpha):\n","        \"\"\"\n","        Actualiza las redes del actor y los críticos utilizando una muestra de transiciones del buffer.\n","\n","        :param batch_size: Tamaño del minibatch de la muestra.\n","        :param gamma: Factor de descuento para las recompensas futuras.\n","        :param tau: Factor de actualización suave para las redes objetivo.\n","        :param alpha: Parámetro de entropía que controla la cantidad de exploración.\n","        \"\"\"\n","        # Solo actualiza si hay suficiente memoria en el buffer.\n","        if len(self.replay_buffer) < batch_size:\n","            return\n","        # Muestra una muestra del buffer.\n","        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n","        # Convierte las muestras a tensores.\n","        state = torch.FloatTensor(state)\n","        next_state = torch.FloatTensor(next_state)\n","        action = torch.FloatTensor(action)\n","        reward = torch.FloatTensor(reward).unsqueeze(1)\n","        done = torch.FloatTensor(np.float32(done)).unsqueeze(1)\n","\n","        # Calcula los valores objetivo Q para el siguiente estado utilizando las redes objetivo.\n","        with torch.no_grad():\n","            next_state_action = self.actor.sample(next_state)\n","            target_q1_next = self.target_critic_1(next_state, next_state_action)\n","            target_q2_next = self.target_critic_2(next_state, next_state_action)\n","            target_q_min = torch.min(target_q1_next, target_q2_next) - alpha * torch.log(1 - next_state_action.pow(2) + 1e-6)\n","            target_q = reward + (1 - done) * gamma * target_q_min\n","\n","        # Actualización de la red crítica 1.\n","        current_q1 = self.critic_1(state, action)\n","        critic_1_loss = F.mse_loss(current_q1, target_q)\n","        self.critic_1_optimizer.zero_grad()\n","        critic_1_loss.backward()\n","        self.critic_1_optimizer.step()\n","\n","        # Actualización de la red crítica 2.\n","        current_q2 = self.critic_2(state, action)\n","        critic_2_loss = F.mse_loss(current_q2, target_q)\n","        self.critic_2_optimizer.zero_grad()\n","        critic_2_loss.backward()\n","        self.critic_2_optimizer.step()\n","\n","        # Actualización de la red del actor.\n","        entropy = torch.log(1 - self.actor.sample(state).pow(2) + 1e-6)\n","        actor_loss = (-self.critic_1(state, self.actor.sample(state)) + alpha * entropy).mean()\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","\n","        # Actualización suave de las redes objetivo críticas.\n","        for target_param, param in zip(self.target_critic_1.parameters(), self.critic_1.parameters()):\n","            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n","        for target_param, param in zip(self.target_critic_2.parameters(), self.critic_2.parameters()):\n","            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n"],"metadata":{"id":"R5mD63EmG7nK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = gym.make(\"CarRacing-v2\")"],"metadata":{"id":"GDxH2aneG9Wr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["agent = SACAgent()"],"metadata":{"id":"kkiB_I_oG-yp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_episodes = 100  # Define el número total de episodios para el entrenamiento.\n","\n","for episode in range(num_episodes):\n","    # Reinicia el entorno para comenzar un nuevo episodio.\n","    state = env.reset()\n","    episode_reward = 0  # Inicializa el total de recompensa del episodio en 0.\n","    done = False  # Marca el estado del episodio como no terminado al principio.\n","\n","    while not done:\n","        # Selecciona una acción basada en el estado actual utilizando el agente.\n","        action = agent.select_action(state)\n","        # Toma la acción en el entorno y recibe el siguiente estado, la recompensa y si el episodio terminó.\n","        next_state, reward, done, _ = env.step(action)\n","        # Almacena la transición (estado, acción, recompensa, siguiente estado, done) en el buffer de repetición.\n","        agent.replay_buffer.push(state, action, reward, next_state, done)\n","        # Actualiza el agente utilizando una muestra del buffer de repetición.\n","        agent.update(batch_size)\n","        # Actualiza el estado actual al siguiente estado.\n","        state = next_state\n","        # Acumula la recompensa obtenida en el episodio.\n","        episode_reward += reward\n","\n","    # Imprime el total de recompensa acumulada al final del episodio.\n","    print(f\"Episode {episode}: Total Reward: {episode_reward}\")\n"],"metadata":{"id":"Tv5v1tqpHBie"},"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}