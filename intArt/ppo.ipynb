{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"HOg-aI7LI6iM"},"outputs":[],"source":["import gymnasium as gym\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import torch.nn.functional as F\n","from torch.distributions import MultivariateNormal"]},{"cell_type":"code","source":["# Estableciendo los hiperparámetros\n","hidden_dim = 256\n","lr_actor = 3e-4\n","lr_critic = 1e-3\n","gamma = 0.99\n","gae_lambda = 0.95\n","ppo_epochs = 10\n","mini_batch_size = 64\n","ppo_clip = 0.2\n","buffer_size = 2048\n","update_timestep = buffer_size\n","action_std = 0.5  # Standard deviation for action exploration"],"metadata":{"id":"7LJo4sqGI7-l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Construyendo la red actor-crítico\n","class ActorCritic(nn.Module):\n","\n","    def __init__(self, num_actions):\n","        super(ActorCritic, self).__init__()\n","        # capas comunes\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n","        self.fc1 = nn.Linear(64 * 7 * 7, hidden_dim)\n","        # capas de actor\n","        self.fc_actor = nn.Linear(hidden_dim, num_actions)\n","        self.log_std = nn.Parameter(torch.zeros(num_actions))\n","        # capas críticas\n","        self.fc_critic = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state):\n","        x = F.relu(self.conv1(state))\n","        x = F.relu(self.conv2(x))\n","        x = F.relu(self.conv3(x))\n","        x = x.view(x.size(0), -1)\n","        x = F.relu(self.fc1(x))\n","        # Actor\n","        action_mean = self.fc_actor(x)\n","        action_var = torch.exp(self.log_std.expand_as(action_mean))\n","        cov_mat = torch.diag_embed(action_var)\n","        # Critic\n","        value = self.fc_critic(x)\n","        return action_mean, cov_mat, value"],"metadata":{"id":"p6sn_WwuJBdD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Implementación del búfer de memoria\n","class Memory:\n","\n","    def __init__(self):\n","        self.actions = []\n","        self.states = []\n","        self.logprobs = []\n","        self.rewards = []\n","        self.is_terminals = []\n","\n","    def clear_memory(self):\n","        del self.actions[:]\n","        del self.states[:]\n","        del self.logprobs[:]\n","        del self.rewards[:]\n","        del self.is_terminals[:]"],"metadata":{"id":"zmlYBmM4JDzr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Construyendo el Agente PPO\n","class PPO:\n","\n","    def __init__(self, num_actions):\n","        self.policy = ActorCritic(num_actions)\n","        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr_actor)\n","        self.policy_old = ActorCritic(num_actions)\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","        self.MseLoss = nn.MSELoss()\n","\n","    def select_action(self, state):\n","        with torch.no_grad():\n","            action_mean, action_var, _ = self.policy_old(state)\n","            dist = MultivariateNormal(action_mean, action_var)\n","            action = dist.sample()\n","            action_logprob = dist.log_prob(action)\n","        return action.detach().numpy(), action_logprob.detach()\n","\n","    def update(self, memory):\n","        # Estimación Monte Carlo de las recompensas\n","        rewards = []\n","        discounted_reward = 0\n","        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n","            if is_terminal:\n","                discounted_reward = 0\n","            discounted_reward = reward + (gamma * discounted_reward)\n","            rewards.insert(0, discounted_reward)\n","        # Normalizar las recoompensas\n","        rewards = torch.tensor(rewards, dtype=torch.float32)\n","        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n","        # Convertir la lista a tensor\n","        old_states = torch.stack(memory.states).detach()\n","        old_actions = torch.stack(memory.actions).detach()\n","        old_logprobs = torch.stack(memory.logprobs).detach()\n","        # Optimizar la política para k épocas\n","        for _ in range(ppo_epochs):\n","            # Evaluar acciones previas\n","            action_means, action_vars, state_values = self.policy(old_states)\n","            dists = MultivariateNormal(action_means, action_vars)\n","            logprobs = dists.log_prob(old_actions)\n","            dist_entropy = -logprobs.mean()\n","            # Encontrar el ratio (pi_theta / pi_theta__old)\n","            ratios = torch.exp(logprobs - old_logprobs.detach())\n","            # Encontrar Surrogate Loss\n","            advantages = rewards - state_values.detach()\n","            surr1 = ratios * advantages\n","            surr2 = torch.clamp(ratios, 1-ppo_clip, 1+ppo_clip) * advantages\n","            loss = -torch.min(surr1, surr2) + 0.5*self.MseLoss(state_values, rewards) - 0.01*dist_entropy\n","            # Obtener gradient step\n","            self.optimizer.zero_grad()\n","            loss.mean().backward()\n","            self.optimizer.step()\n","        # Copiar nuevos pesos a la nueva política\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n"],"metadata":{"id":"61MwCieHJHDH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Preprocesamiento de los estados\n","def preprocess_state(state):\n","    state = np.ascontiguousarray(state, dtype=np.float32) / 255\n","    state = torch.from_numpy(state)\n","    return state.unsqueeze(0)"],"metadata":{"id":"qXVef8zwJJxb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Configurar el entorno\n","env = gym.make('CarRacing-v2')"],"metadata":{"id":"tFF3TFiQJLcR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creando la memoria\n","memory = Memory()"],"metadata":{"id":"RDFL6K4sJM71"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creando el agente\n","ppo = PPO(env.action_space.shape[0])"],"metadata":{"id":"nl5w07OLJOoj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Implementación del circuito de entrenamiento\n","state = env.reset()\n","state = preprocess_state(state)\n","for t in range(1, update_timestep+1):\n","    action, action_logprob = ppo.select_action(state)\n","    next_state, reward, done, _ = env.step(action)\n","    next_state = preprocess_state(next_state)\n","    memory.states.append(state)\n","    memory.actions.append(torch.tensor(action))\n","    memory.logprobs.append(action_logprob)\n","    memory.rewards.append(reward)\n","    memory.is_terminals.append(done)\n","    state = next_state\n","    if done:\n","        state = env.reset()\n","        state = preprocess_state(state)\n","    if t % update_timestep == 0:\n","        ppo.update(memory)\n","        memory.clear_memory()"],"metadata":{"id":"PfgSjtAbJQvo"},"execution_count":null,"outputs":[]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}